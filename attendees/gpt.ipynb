{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, TypedDict\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import os\n",
    "from backoff import on_exception, expo\n",
    "from openai.error import RateLimitError, APIError, InvalidRequestError\n",
    "import pandas as pd\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class Message(TypedDict):\n",
    "    \"\"\"OpenAI Message object containing a role and the message content\"\"\"\n",
    "\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "class OpenAI:\n",
    "    def __init__(self, relevant_memory, full_message_history):\n",
    "        self.relevant_memory = relevant_memory\n",
    "        self.full_message_history = full_message_history\n",
    "        self.next_message_to_add_index = 0\n",
    "        self.current_tokens_used = 0\n",
    "        self.insertion_index = 0\n",
    "        self.current_context = []\n",
    "        self.response = None\n",
    "\n",
    "    @on_exception(expo, InvalidRequestError, max_tries=3)\n",
    "    def get_openai_response(self, messages, model, temperature, max_tokens):\n",
    "        COMPLETIONS_API_PARAMS = {\n",
    "            # We use temperature of 0.0 because it gives the most predictable, factual answer.\n",
    "            \"temperature\": temperature,\n",
    "            \"model\": model,\n",
    "            \"max_tokens\": max_tokens,\n",
    "        }\n",
    "        response = openai.ChatCompletion.create(\n",
    "            messages=messages, **COMPLETIONS_API_PARAMS)\n",
    "        return response\n",
    "\n",
    "    def create_chat_completion(\n",
    "        self,\n",
    "        messages: List[Message],  # type: ignore\n",
    "        model: Optional[str] = None,\n",
    "        temperature: float = 0.0,\n",
    "        max_tokens: Optional[int] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Create a chat completion using the OpenAI API\n",
    "\n",
    "        Args:\n",
    "            messages (List[Message]): The messages to send to the chat completion\n",
    "            model (str, optional): The model to use. Defaults to None.\n",
    "            temperature (float, optional): The temperature to use. Defaults to 0.9.\n",
    "            max_tokens (int, optional): The max tokens to use. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            str: The response from the chat completion\n",
    "        \"\"\"\n",
    "        num_retries = 3\n",
    "        response = None\n",
    "        for attempt in range(num_retries):\n",
    "            backoff = 2 ** (attempt + 2)\n",
    "            try:\n",
    "                response = self.get_openai_response(\n",
    "                    messages, model, temperature, max_tokens)\n",
    "                break\n",
    "            except RateLimitError as e:\n",
    "                print(e)\n",
    "            except APIError as e:\n",
    "                if e.http_status != 502:\n",
    "                    raise\n",
    "                if attempt == num_retries - 1:\n",
    "                    raise\n",
    "            time.sleep(backoff)\n",
    "        if response is None:\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to get response after {num_retries} retries\")\n",
    "        resp = response.choices[0].message[\"content\"]\n",
    "        return resp\n",
    "\n",
    "    def generate_chat_context(self, prompt):\n",
    "        current_context = [\n",
    "            self.create_chat_message(\"system\", prompt),\n",
    "            self.create_chat_message(\n",
    "                \"system\", f\"The current time and date is {time.strftime('%c')}\"\n",
    "            ),\n",
    "            self.create_chat_message(\n",
    "                \"system\",\n",
    "                f\"This reminds you of these events from your past:\\n{self.relevant_memory}\\n\\n\",\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        # Add messages from the full message history until we reach the token limit\n",
    "        next_message_to_add_index = len(self.full_message_history) - 1\n",
    "        insertion_index = len(current_context)\n",
    "        # Count the currently used tokens\n",
    "        current_tokens_used = [self.count_tokens(context['content'])\n",
    "                                for context in current_context]\n",
    "        return (\n",
    "            next_message_to_add_index,\n",
    "            current_tokens_used,\n",
    "            insertion_index,\n",
    "            current_context,\n",
    "        )\n",
    "\n",
    "    def create_chat_message(self, role, content) -> Message:\n",
    "        return {\"role\": role, \"content\": content}\n",
    "\n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"count the number of tokens in a string\"\"\"\n",
    "        return len(tokenizer.encode(text))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ice-breaker-kYPj9F3u",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
